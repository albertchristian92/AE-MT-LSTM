{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde58071",
   "metadata": {},
   "source": [
    "### Code yang diperlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1041132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16515514961516078718\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# split data to 14001 file, reference: https://stackoverflow.com/questions/31786287/how-to-split-large-text-file-in-windows\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from network_module import dense_block, transition_block, Conv1DTranspose\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "62861f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_DATA = 14001  # Hardcode jumlah data training\n",
    "BATCH_SIZE = 2  # Hardcode batch size\n",
    "# wifi_train_data = np.loadtxt(\"data0601_LPS_2/train/w_train_data.txt\").reshape(14001, 21, 15)\n",
    "# mag_train_data = np.loadtxt(\"data0601_LPS_2/train/m_train_data.txt\").reshape(14001, 21, 2)\n",
    "# gps_train_data = np.loadtxt(\"data0601_LPS_2/train/g_train_data.txt\").reshape(14001, 21, 4)\n",
    "# tr_labelc = np.loadtxt(\"data0601_LPS_2/train/train_labelc.txt\").reshape(14001, 21, 2)\n",
    "# tr_label = np.loadtxt(\"data0601_LPS_2/train/train_label.txt\").reshape(14001, 21)\n",
    "# tr_wgs_label = np.loadtxt(\"data0601_LPS_2/train/wgs_label.txt\").reshape(14001, 21, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419eef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gps_model(path):\n",
    "    inp = tf.keras.layers.Input(shape=(4), name='input_layer1')\n",
    "    model = tf.keras.layers.Reshape((4,1))(inp)\n",
    "    model = tf.keras.layers.Conv1D(32,3)(model)\n",
    "    model = tf.keras.layers.AveragePooling1D(strides=1,padding=\"same\")(model)\n",
    "    model = dense_block(32,3,3,4)(model)\n",
    "    model = transition_block(stride=1)(model)\n",
    "    model = dense_block(32,3,3,4)(model)\n",
    "    model_down_gps = tf.keras.Model(inputs=[inp], outputs=model)\n",
    "    # load gps trained model\n",
    "    latest = tf.train.latest_checkpoint(path)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model_down=model_down_gps)\n",
    "    checkpoint.restore(latest).expect_partial()\n",
    "    return model_down_gps\n",
    "\n",
    "def load_wifi_model(path):\n",
    "    inp = tf.keras.layers.Input(shape=(15), name='input_layer1')\n",
    "    model = tf.keras.layers.Reshape((15,1))(inp)\n",
    "    model = tf.keras.layers.Conv1D(32,3)(model)\n",
    "    model = tf.keras.layers.AveragePooling1D(strides=1)(model)\n",
    "    model = dense_block(32,3,3,4)(model)\n",
    "    model = transition_block(stride=1)(model)\n",
    "    model_down_wifi = tf.keras.Model(inputs=[inp], outputs=model)\n",
    "\n",
    "    path = \"./wifi_checkpoints/checkpoints_0524_conv/\"\n",
    "    latest = tf.train.latest_checkpoint(path)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model_down=model_down_wifi)\n",
    "    checkpoint.restore(latest).expect_partial()\n",
    "    return model_down_wifi\n",
    "\n",
    "def gps_preprocess(model_down_gps, gps_train_data):\n",
    "    #print(\"gps preprocess\")\n",
    "    #memasukkan data ke dalam model\n",
    "    GPS = model_down_gps(gps_train_data.astype('float32'))  # GPS = tf.keras.Model(inputs=[gps_train_data.astype('float32')], outputs=model)\n",
    "    #reshape data menjadi data 2D dengan baris shape GPS.shape[0], kolomnya otomatis -> makanya pakai -1\n",
    "    train_GPS = np.array(GPS).reshape(GPS.shape[0],-1)\n",
    "    #print(\"gps shape : \", train_GPS[0].shape)\n",
    "\n",
    "    #return data yang sudah dipre-process dan direshape -> sama dengan fungsi gps_preprocess\n",
    "    return train_GPS\n",
    "\n",
    "def wifi_preprocess(model_down_wifi, wifi_train_data):\n",
    "    #print(\"wifi preprocess\")\n",
    "    wifi = model_down_wifi(wifi_train_data.astype('float32'))\n",
    "\n",
    "    #reshape data menjadi data 2D dengan baris shape wifi.shape[0], kolomnya otomatis -> makanya pakai -1\n",
    "    train_wifi = np.array(wifi).reshape(wifi.shape[0],-1)\n",
    "    #print(\"wifi shape : \", train_wifi[0].shape)\n",
    "    \n",
    "    #return data yang sudah dipre-process dan direshape -> sama dengan fungsi wifi_preprocess\n",
    "    return train_wifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "713b7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_down_gps = load_gps_model(path=\"./gps_checkpoint-002/gps_checkpoints/checkpoints_0601_LPS_2/\")\n",
    "model_down_wifi = load_wifi_model(path=\"./wifi_checkpoints/checkpoints_0524_conv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4516bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    for i in range(TOTAL_DATA):\n",
    "        train_data = []\n",
    "        train_label = []\n",
    "        train_iolabel = []\n",
    "        train_wgs_label = []\n",
    "        \n",
    "        # asumsi start data dari 0\n",
    "        wifi_train_data = np.loadtxt(f\"./data0601_LPS_2/train/w_train_data/w_train_{str(i).zfill(5)}.txt\").reshape(21, 15)\n",
    "        wifi = wifi_preprocess(model_down_wifi, wifi_train_data)\n",
    "\n",
    "        g_mag = np.loadtxt(f\"data0601_LPS_2/train/m_train_data/m_train_{str(i).zfill(5)}.txt\").reshape(21, 2)\n",
    "        gps_train_data = np.loadtxt(f\"./data0601_LPS_2/train/g_train_data/g_train_{str(i).zfill(5)}.txt\").reshape(21, 4)\n",
    "        GPS = gps_preprocess(model_down_gps, gps_train_data)\n",
    "        lstm_input = np.concatenate([wifi, g_mag, GPS], 1)\n",
    "        \n",
    "        tr_label = np.loadtxt(f\"./data0601_LPS_2/train/train_label/train_label_{str(i).zfill(5)}.txt\").reshape(21,)\n",
    "        tr_labelc = np.loadtxt(f\"./data0601_LPS_2/train/train_labelc/train_labelc_{str(i).zfill(5)}.txt\").reshape(21, 2)\n",
    "        tr_wgs_label = np.loadtxt(f\"./data0601_LPS_2/train/wgs_label/wgs_label_{str(i).zfill(5)}.txt\").reshape(21, 15)\n",
    "        \n",
    "        for j in range(17):\n",
    "            train_data.append(lstm_input[j:j+5])\n",
    "            train_label.append(tr_labelc[j+4])\n",
    "            if tr_label[j+4] < 26:\n",
    "                train_iolabel.append(0)\n",
    "            else:\n",
    "                train_iolabel.append(1)\n",
    "            train_wgs_label.append(np.array(tr_wgs_label[j:j+5]).reshape(-1))\n",
    "\n",
    "        train_data = np.array(train_data).astype('float32')\n",
    "        train_label = np.array(train_label).astype('float32')\n",
    "        io_label = np.array(train_iolabel).astype('int32')\n",
    "        wgs_label = np.array(train_wgs_label).astype('int32')\n",
    "        io_label = tf.one_hot(io_label, 2)\n",
    "\n",
    "        #print(train_data.shape, train_label.shape, io_label.shape, wgs_label.shape)\n",
    "        yield (train_data, train_label, io_label, wgs_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "32901442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    lstm_input = tf.keras.layers.Input(shape=(5,866), name='input_layer_lstm')\n",
    "    lstm1 = tf.keras.layers.LSTM(20,return_sequences=True)(lstm_input)\n",
    "    lstm2 = tf.keras.layers.LSTM(20,return_sequences=False)(lstm1)\n",
    "    fc3 = tf.keras.layers.Dense(2,name='fc3')(lstm2)\n",
    "    io3 = tf.keras.layers.Dense(2,activation='sigmoid',name='io3')(lstm2)\n",
    "    wgs3 = tf.keras.layers.Dense(75,activation='sigmoid',name='wgs3')(lstm2)\n",
    "    model_lstm = tf.keras.Model(inputs=[lstm_input], outputs=[fc3,io3,wgs3],name = \"lstm\")\n",
    "\n",
    "    return model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67c2102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "optimizer_A = tf.optimizers.Nadam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebd554d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer_A, t_x, t_y, t_io, t_wgs):\n",
    "    lstm_vars = model.trainable_variables\n",
    "\n",
    "    with tf.GradientTape() as lstm_tape, tf.GradientTape() as io_tape, tf.GradientTape() as wgs_tape:\n",
    "        output = model(t_x, training=True)\n",
    "        coor_loss, io_loss, wgs_loss = model_loss(output,t_y,t_io,t_wgs)\n",
    "        coor_loss = tf.multiply(0.5,coor_loss)\n",
    "\n",
    "    # rand = np.random.rand()\n",
    "    # if rand < 0.4:\n",
    "    gradients = lstm_tape.gradient([coor_loss,io_loss,wgs_loss], lstm_vars)# + categorial_vars)\n",
    "    optimizer_A.apply_gradients(zip(gradients, lstm_vars))# + categorial_vars))\n",
    "\n",
    "    return np.array(coor_loss).mean(),np.array(io_loss).mean(),np.array(wgs_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d97957e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def model_loss(output, t_y, t_io, t_wgs):\n",
    "    output_categorial , output_io, output_wgs = output\n",
    "    coor_loss = euclidean_distance_loss(t_y,output_categorial)\n",
    "    io_loss = losses.binary_crossentropy(t_io,output_io)\n",
    "    wgs_loss = losses.binary_crossentropy(t_wgs,output_wgs)\n",
    "\n",
    "    return coor_loss, io_loss, wgs_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7bd6430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer_A, epochs):\n",
    "    # Lokasi weight\n",
    "    checkpoint_dir = './lstm_checkpoints/AE_LSTM_checkpoints_0601_coor_LPS_2'\n",
    "    # save weight berdasar checkpoint\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_dropout_model_{epoch}\")\n",
    "    checkpoint = tf.train.Checkpoint(optimizerA=optimizer_A, model_lstm=model)\n",
    "\n",
    "    #save log\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = './tensorflow/logs/gradient_tape/' + current_time + '/train'\n",
    "    test_log_dir = './tensorflow/logs/gradient_tape/' + current_time + '/test'\n",
    "    # save log jadi file yang compatible dengan tensorboard sehingga bisa divisualisasikan\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "    # berkaitan dengan lost function\n",
    "    minimum_delta = 0.0001\n",
    "    history = {}\n",
    "    history['val_loss'] = np.zeros(epochs)\n",
    "\n",
    "    dset = tf.data.Dataset.from_generator(get_train_data,\n",
    "                                      (tf.float32,  # Data type for \"train_data\"\n",
    "                                       tf.float32,  # Data type for \"train_label\"\n",
    "                                       tf.int32,  # Data type for \"io_label\"\n",
    "                                       tf.int32),  # Data type for \"wgs_label\"\n",
    "                                     )\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch : \", epoch)\n",
    "        start = time.time()\n",
    "        all_coor = []\n",
    "        all_io = []\n",
    "        all_wgs = []\n",
    "\n",
    "        data = next(iter(dset.shuffle(5).batch(BATCH_SIZE)))\n",
    "\n",
    "        train_data, train_label, io_label, wgs_label = data\n",
    "        for h in range(BATCH_SIZE):\n",
    "            for i in range(17):\n",
    "                # Variable `data` in merupakan tuple (train_data, train_label, io_label, wgs_label)\n",
    "                # Dimana masing-masing memiliki shape 4-dimensi, kita pakai contoh BATCH_SIZE = 1\n",
    "                # Shape dari x/train_data == (1, 17, 5, 866)\n",
    "                # Shape dari y/train_label == (1, 17, 2)\n",
    "                # Shape dari io/io_label == (1, 17, 2)\n",
    "                # Shape dari wgs/wgs_label == (1, 17, 75)\n",
    "\n",
    "                x = tf.expand_dims(train_data[h][i], axis=0)  # Perlu expand_dims supaya seolah-olah data ini jadi memiliki shape (-1, 5, 866), jika tidak maka hanya akan menjadi (5, 866)\n",
    "                y = tf.expand_dims(train_label[h][i], axis=0)\n",
    "                io = tf.expand_dims(io_label[h][i], axis=0)\n",
    "                wgs = tf.expand_dims(wgs_label[h][i], axis=0)\n",
    "                coor_loss,io_loss,wgs_loss = train_step(model, optimizer_A, x,y,io,wgs)\n",
    "                \n",
    "                # print(x.shape)\n",
    "                all_coor.append(coor_loss)\n",
    "                all_io.append(io_loss)\n",
    "                all_wgs.append(wgs_loss)\n",
    "        \n",
    "        print(\"train categorial loss : {}, train io loss : {}, train wgs loss : {}\".format(np.array(all_coor).mean(),np.array(all_io).mean(),np.array(all_wgs).mean()))\n",
    "\n",
    "        # train tulis ke file supaya bisa dibaca di tensorboard\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('coor loss', np.array(all_coor).mean(), step=epoch)\n",
    "            tf.summary.scalar('io loss', np.array(all_io).mean(), step=epoch)\n",
    "            tf.summary.scalar('wgs loss', np.array(all_wgs).mean(), step=epoch)\n",
    "\n",
    "        # for (val_x, val_y, val_io, val_wgs) in self.test_dataset:\n",
    "        #     loss, io_loss, wgs_loss = self.validation(val_x, val_y, val_io, val_wgs)\n",
    "            # print(\"val end\")\n",
    "        \n",
    "        # with test_summary_writer.as_default():\n",
    "        #     tf.summary.scalar('coor loss', loss, step=epoch)\n",
    "        #     tf.summary.scalar('io loss', io_loss, step=epoch)\n",
    "        #     tf.summary.scalar('wgs loss', wgs_loss, step=epoch)\n",
    "\n",
    "        # print(\"sum end\")\n",
    "        # history['val_loss'][epoch] = loss\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        # if epoch > 200: #early stop\n",
    "        #     differences = np.abs(np.diff(history['val_loss'][epoch - 5:epoch], n = 1))\n",
    "        #     check =  differences > minimum_delta \n",
    "\n",
    "        #     if np.all(check == False):\n",
    "        #         print(differences)\n",
    "        #         print(\"\\n\\nEarlyStopping Evoked! Stopping training\\n\\n\")\n",
    "        #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "62dc855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "train categorial loss : 1.0012296438217163, train io loss : 0.3453819155693054, train wgs loss : 0.458695650100708\n",
      "epoch :  1\n",
      "train categorial loss : 1.012282371520996, train io loss : 0.3894517123699188, train wgs loss : 0.5302849411964417\n",
      "epoch :  2\n",
      "train categorial loss : 0.8670793175697327, train io loss : 0.30510184168815613, train wgs loss : 0.5015010237693787\n",
      "epoch :  3\n",
      "train categorial loss : 0.9131344556808472, train io loss : 0.22039832174777985, train wgs loss : 0.39947906136512756\n",
      "epoch :  4\n",
      "train categorial loss : 0.7445359230041504, train io loss : 0.18408595025539398, train wgs loss : 0.38137057423591614\n",
      "epoch :  5\n",
      "train categorial loss : 0.9210472702980042, train io loss : 0.29982325434684753, train wgs loss : 0.5165290832519531\n",
      "epoch :  6\n",
      "train categorial loss : 0.9136998653411865, train io loss : 0.18729694187641144, train wgs loss : 0.4023730158805847\n",
      "epoch :  7\n",
      "train categorial loss : 0.8477675914764404, train io loss : 0.15233919024467468, train wgs loss : 0.3207407295703888\n",
      "epoch :  8\n",
      "train categorial loss : 0.757789671421051, train io loss : 0.17034129798412323, train wgs loss : 0.4019799828529358\n",
      "epoch :  9\n",
      "train categorial loss : 0.9714520573616028, train io loss : 0.17084887623786926, train wgs loss : 0.37142589688301086\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer_A, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a310f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dset.shuffle(5).batch(2)))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(batch_size):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    train_iolabel = []\n",
    "    train_wgs_label = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        wifi = train_wifi[i]\n",
    "        g_mag = mag_train_data[i]\n",
    "        GPS = train_GPS[i]\n",
    "        lstm_input = np.concatenate([wifi, g_mag, GPS], 1)\n",
    "        for j in range(17):\n",
    "            train_data.append(lstm_input[j:j+5])\n",
    "            train_label.append(tr_labelc[i][j+4])\n",
    "            if tr_label[i][j+4] < 26:\n",
    "                train_iolabel.append(0)\n",
    "            else:\n",
    "                train_iolabel.append(1)\n",
    "            train_wgs_label.append(np.array(tr_wgs_label[i][j:j+5]).reshape(-1))\n",
    "\n",
    "    train_data = np.array(train_data).astype('float32')\n",
    "    train_label = np.array(train_label).astype('float32')\n",
    "    io_label = np.array(train_iolabel).astype('int32')\n",
    "    wgs_label = np.array(train_wgs_label).astype('int32')\n",
    "    io_label = tf.one_hot(io_label, 2)\n",
    "\n",
    "    print(train_data.shape, train_label.shape, io_label.shape, wgs_label.shape)\n",
    "    yield (train_data, train_label, io_label, wgs_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Tensorflow dataset object \"dset\"\n",
    "dset = tf.data.Dataset.from_generator(get_train_data,\n",
    "                                      (tf.float32,  # Data type for \"train_data\"\n",
    "                                       tf.float32,  # Data type for \"train_label\"\n",
    "                                       tf.int32,  # Data type for \"io_label\"\n",
    "                                       tf.int32),  # Data type for \"wgs_label\"\n",
    "                                      args=[BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_data, train_label, io_label, wgs_label in dset:\n",
    "    print(train_data.shape, train_label.shape, io_label.shape, wgs_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec551a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for train_data, train_label, io_label, wgs_label in dset:\n",
    "    print(f\"train_data.numpy:\\n{train_data.numpy()}\")\n",
    "    print(f\"train_label.numpy:\\n{train_label.numpy()}\")\n",
    "    print(f\"io_label.numpy:\\n{io_label.numpy()}\")\n",
    "    print(f\"wgs_label.numpy:\\n{wgs_label.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1274055",
   "metadata": {},
   "source": [
    "### Testing, mohon abaikan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(stop):\n",
    "    i = 0\n",
    "    while i<stop:\n",
    "        yield i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717d559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch: panjang per data yang diambil, misal batch(10) berarti per data panjangnya 10\n",
    "# Take: berapa banyak data yang diambil, misal take(10) berarti ambil 10 data dari generator\n",
    "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
    "    print(count_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count_batch in ds_counter.repeat().batch(2).take(10):\n",
    "    print(count_batch.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
